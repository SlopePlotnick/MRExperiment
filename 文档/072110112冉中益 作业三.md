# 模式识别 作业三

本次作业的课题是研究马尔可夫模型在时序预测模型中的应用。秉持着`Talk is cheap.Show me the code.`的原则，我决定以项目代码导向型的方式进行自主学习，并在学习后针对课堂问题进行回答

## 1. 自主学习

### 1.1 基础介绍

马尔可夫模型可以用如下的模型图直观地进行表示：

![模型图](/Users/plotnickslope/Desktop/学习资料/模式识别/作业/文档/1366772946_8884.png)

第一排是y序列，第二排是x序列。每个x都只有一个y指向它，每个y也都有另一个y指向它。

参考《统计学习方法》给出明确定义：

- **状态序列（上图中的y，下面的I）**： 隐藏的马尔科夫链随机生成的状态序列，称为状态序列（state sequence）
- **观测序列（上图中的x，下面的O）**: 每个状态生成一个观测，而由此产生的观测的随机序列，称为观测序列（obeservation sequence）
- **马尔科夫模型**： 马尔科夫模型是关于时序的概率模型，描述由一个隐藏的马尔科夫链**随机生成不可观测的状态随机序列，再由各个状态生成一个观测而产生观测随机序列**的过程。

#### 1.1.1 形式定义

设Q是所有可能的状态的集合，V是所有可能的观测的集合。

Q=q1,q2,...,qN

V=v1,v2,...,vM

其中，N是可能的状态数，M是可能的观测数。

I是长度为T的状态序列，O是对应的观测序列。

I=(i1,i2,...,iT)

O=(o1,o2,...,oT)

**A是状态转移矩阵**：A=[aij]N×N

i=1,2,...,N

j=1,2,...,N

其中，在时刻t，处于qi状态的条件下在时刻t+1转移到状态qj的概率：

aij=P(it+1=qj|it=qi)

**B是观测概率矩阵**：B=[bj(k)]N×M

k=1,2,...,M

j=1,2,...,N

其中，在时刻t处于状态qj的条件下生成观测vk的概率：

bj(k)=P(ot=vk|it=qj)

**π是初始状态概率向量**：π=(πi)

其中，πi=P(i1=qi)

隐马尔科夫模型由初始状态概率向量π、状态转移概率矩阵A和观测概率矩阵B决定。π和A决定状态序列，B决定观测序列。因此，隐马尔科夫模型λ可以由三元符号表示，即：λ=(A,B,π)。A,B,π称为隐马尔科夫模型的**三要素**。

#### 1.1.2 隐马尔科夫模型的两个基本假设

1. 设隐马尔科夫链在任意时刻tt的状态只依赖于**其前一时刻**的状态，与其他时刻的状态及观测无关，也与时刻tt无关。（**齐次马尔科夫性假设**）

2. 假设任意时刻的观测只依赖于该时刻的马尔科夫链的状态，与其他观测和状态无关。（**观测独立性假设**）

#### 1.1.3 实例

假设你是一个医生，眼前有个病人，你的任务是确定他是否得了感冒。

- 首先，病人的状态(Q)只有两种：{感冒，没有感冒}。
- 然后，病人的感觉（观测V）有三种：{正常，冷，头晕}。
- 手头有病人的病例，你可以从病例的第一天确定π（初始状态概率向量）；
- 然后根据其他病例信息，确定A（状态转移矩阵）也就是病人某天是否感冒和他第二天是否感冒的关系；
- 还可以确定B（观测概率矩阵）也就是病人某天是什么感觉和他那天是否感冒的关系。

条件图示如下：

![条件](/Users/plotnickslope/Desktop/学习资料/模式识别/作业/文档/hmm.jpg)

求解代码如下：

```python
import numpy as np

# 对应状态集合Q
states = ('Healthy', 'Fever')
# 对应观测集合V
observations = ('normal', 'cold', 'dizzy')
# 初始状态概率向量π
start_probability = {'Healthy': 0.6, 'Fever': 0.4}
# 状态转移矩阵A
transition_probability = {
    'Healthy': {'Healthy': 0.7, 'Fever': 0.3},
    'Fever': {'Healthy': 0.4, 'Fever': 0.6},
}
# 观测概率矩阵B
emission_probability = {
    'Healthy': {'normal': 0.5, 'cold': 0.4, 'dizzy': 0.1},
    'Fever': {'normal': 0.1, 'cold': 0.3, 'dizzy': 0.6},
}

# 随机生成观测序列和状态序列    
def simulate(T):

    def draw_from(probs):
        """
        1.np.random.multinomial:
        按照多项式分布，生成数据
        >>> np.random.multinomial(20, [1/6.]*6, size=2)
                array([[3, 4, 3, 3, 4, 3],
                       [2, 4, 3, 4, 0, 7]])
         For the first run, we threw 3 times 1, 4 times 2, etc.  
         For the second, we threw 2 times 1, 4 times 2, etc.
        2.np.where:
        >>> x = np.arange(9.).reshape(3, 3)
        >>> np.where( x > 5 )
        (array([2, 2, 2]), array([0, 1, 2]))
        """
        return np.where(np.random.multinomial(1,probs) == 1)[0][0]

    observations = np.zeros(T, dtype=int)
    states = np.zeros(T, dtype=int)
    states[0] = draw_from(pi)
    observations[0] = draw_from(B[states[0],:])
    for t in range(1, T):
        states[t] = draw_from(A[states[t-1],:])
        observations[t] = draw_from(B[states[t],:])
    return observations, states

def generate_index_map(lables):
    id2label = {}
    label2id = {}
    i = 0
    for l in lables:
        id2label[i] = l
        label2id[l] = i
        i += 1
    return id2label, label2id
 
states_id2label, states_label2id = generate_index_map(states)
observations_id2label, observations_label2id = generate_index_map(observations)
print(states_id2label, states_label2id)
print(observations_id2label, observations_label2id)
{0: 'Healthy', 1: 'Fever'} {'Healthy': 0, 'Fever': 1}
{0: 'normal', 1: 'cold', 2: 'dizzy'} {'normal': 0, 'cold': 1, 'dizzy': 2}

def convert_map_to_vector(map_, label2id):
    """将概率向量从dict转换成一维array"""
    v = np.zeros(len(map_), dtype=float)
    for e in map_:
        v[label2id[e]] = map_[e]
    return v

 
def convert_map_to_matrix(map_, label2id1, label2id2):
    """将概率转移矩阵从dict转换成矩阵"""
    m = np.zeros((len(label2id1), len(label2id2)), dtype=float)
    for line in map_:
        for col in map_[line]:
            m[label2id1[line]][label2id2[col]] = map_[line][col]
    return m

A = convert_map_to_matrix(transition_probability, states_label2id, states_label2id)
B = convert_map_to_matrix(emission_probability, states_label2id, observations_label2id)
observations_index = [observations_label2id[o] for o in observations]
pi = convert_map_to_vector(start_probability, states_label2id)

# 生成模拟数据
observations_data, states_data = simulate(10)
print(observations_data)
print(states_data)
# 相应的label
print("病人的状态: ", [states_id2label[index] for index in states_data])
print("病人的观测: ", [observations_id2label[index] for index in observations_data])
```

求解结果如下：

```
[0 0 1 1 2 1 2 2 2 0]
[0 0 0 0 1 1 1 1 1 0]
病人的状态:  ['Healthy', 'Healthy', 'Healthy', 'Healthy', 'Fever', 'Fever', 'Fever', 'Fever', 'Fever', 'Healthy']
病人的观测:  ['normal', 'normal', 'cold', 'cold', 'dizzy', 'cold', 'dizzy', 'dizzy', 'dizzy', 'normal']
```

### 1.2 HMM的三个问题

1. **概率计算问题**：给定模型λ=(A,B,π) 和观测序列O=o1,o2,...,oT，计算在模型λ下观测序列O出现的概率P(O|λ)。

2. **学习问题**：已知观测序列O=o1,o2,...,oT，估计模型λ=(A,B,π)，使P(O|λ)最大。即用**极大似然法**的方法估计参数。

3. **预测问题**（也称为解码（decoding）问题）：已知观测序列O=o1,o2,...,oT 和模型λ=(A,B,π)，求给定观测序列条件概率P(I|O)最大的状态序列I=(i1,i2,...,iT)，即给定观测序列，求最有可能的对应的**状态序列**。

对应于1.1.3中的实例，这三个问题可以具像化为：

1. **概率计算问题**：如果给定模型参数，病人某一系列观测的症状出现的概率。

2. **学习问题**：根据病人某一些列观测的症状，学习模型参数。

3. **预测问题**：根据学到的模型，预测病人这几天是不是有感冒

下面分别对三个问题的代码求解作讨论

#### 1.2.1 概率计算问题

**直接计算**：

对于状态序列I=(i1,i2,...,iT)的概率是：P(I|λ)=πi1ai1i2ai2i3...aiT−1iT。

对上面这种状态序列，产生观测序列O=(o1,o2,...,oT)的概率是P(O|I,λ)=bi1(o1)bi2(o2)...biT(oT)。

I和O的**联合概率**为P(O,I|λ)=P(O|I,λ)P(I|λ)=πi1bi1(o1)ai1i2bi2(o2)...aiT−1iTbiT(oT)。

对所有可能的I求和，得到P(O|λ)=∑IP(O,I|λ)=∑i1,...,iTπi1bi1(o1)ai1i2bi2(o2)...aiT−1iTbiT(oT)。

如果直接计算，时间复杂度太高，是O(TN^T^)。



**前向算法**：

首先引入**前向概率**：

给定模型λ，定义到时刻t部分观测序列为o1,o2,...,ot且状态为qi的概率为前向概率。记作：

αt(i)=P(o1,o2,...,ot,it=qi|λ)

用感冒例子描述就是：某一天是否感冒以及这天和这天之前所有的观测症状的联合概率。

后向概率定义类似。

**前向算法描述**

输入：隐马模型λ，观测序列O; 输出：观测序列概率P(O|λ).

1. 初值：(t=1)，α1(i)=P(o1,i1=q1|λ)=πibi(o1)，i=1,2,...,N

2. 递推：对t=1,2,...,N，αt+1(i)=[∑j=1Nαt(j)aji]bi(ot+1)

3. 终结：P(O|λ)=∑i=1NαT(i)

**理解：**

前向算法使用**前向概率**的概念，记录每个时间下的前向概率，使得在递推计算下一个前向概率时，只需要上一个时间点的所有前向概率即可。原理上也是用空间换时间。这样的**时间复杂度是O(N^2^T)**。



**算法实现：**

```python
def forward(obs_seq):
    """前向算法"""
    N = A.shape[0]
    T = len(obs_seq)
    
    # F保存前向概率矩阵
    F = np.zeros((N,T))
    F[:,0] = pi * B[:, obs_seq[0]]

    for t in range(1, T):
        for n in range(N):
            F[n,t] = np.dot(F[:,t-1], (A[:,n])) * B[n, obs_seq[t]]

    return F

def backward(obs_seq):
    """后向算法"""
    N = A.shape[0]
    T = len(obs_seq)
    # X保存后向概率矩阵
    X = np.zeros((N,T))
    X[:,-1:] = 1

    for t in reversed(range(T-1)):
        for n in range(N):
            X[n,t] = np.sum(X[:,t+1] * A[n,:] * B[:, obs_seq[t+1]])

    return X
```

#### 1.2.2 学习问题

有监督的学习算法在有标注数据的前提下，使用**极大似然估计法**可以很方便地估计模型参数，此处不作细致讨论。

#### 1.2.3 预测问题

考虑到预测问题是求给定观测序列条件概率P(I|O)最大的状态序列I=(i1,i2,...,iT)，类比这个问题和最短路问题：

我们可以把求P(I|O)的最大值类比成求节点间距离的最小值，于是考虑**类似于动态规划的viterbi算法**。

**首先导入两个变量δ和ψ**：

定义**在时刻t状态为i的所有单个路径(i1,i2,i3,...,it)中概率最大值**为(这里考虑P(I,O)便于计算，因为给定的P(O),P(I|O)正比于P(I,O):

δt(i)=maxi1,i2,...,it−1P(it=i,it−1,...,i1,ot,ot−1,...,o1|λ)

读作delta，其中，i=1,2,...,N

得到其递推公式：

δt(i)=max1≤j≤N[δt−1(j)aji]bi(o1)

定义**在时刻t状态为i的所有单个路径(i1,i2,i3,...,it−1,i)中概率最大的路径的第t−1个结点**为

ψt(i)=argmax1≤j≤N[δt−1(j)aji]

读作psi，其中，i=1,2,...,N

下面介绍维特比算法。

**维特比（viterbi）算法**（动态规划）：

输入：模型λ=(A,B,π)和观测O=(o1,o2,...,oT)

输出：最优路径I∗=(i1∗,i2∗,...,iT∗)

1. 初始化：

δ1(i)=πibi(o1)

ψ1(i)=0

2. **递推。**对t=2,3,...,T

δt(i)=max1≤j≤N[δt−1(j)aji]bi(ot)

ψt(i)=argmax1≤j≤N[δt−1(j)aji]

3. 终止：

P∗=max1≤i≤NδT(i)

iT*=argmax1≤i≤NδT(i)

(4).最优路径回溯，对t=T−1,T−2,...,1

it∗=ψt+1(it+1∗)

求得最优路径I∗=(i∗1,i∗2,...,i∗T)

**注：上面的bi(ot)和ψt+1(it+1∗)的括号，并不是函数，而是类似于数组取下标的操作。**



**算法实现：**

```python
def viterbi(obs_seq, A, B, pi):
    """
    Returns
    -------
    V : numpy.ndarray
        V [s][t] = Maximum probability of an observation sequence ending
                   at time 't' with final state 's'
    prev : numpy.ndarray
        Contains a pointer to the previous state at t-1 that maximizes
        V[state][t]
        
    V对应δ，prev对应ψ
    """
    N = A.shape[0]
    T = len(obs_seq)
    prev = np.zeros((T - 1, N), dtype=int)

    # DP matrix containing max likelihood of state at a given time
    V = np.zeros((N, T))
    V[:,0] = pi * B[:,obs_seq[0]]

    for t in range(1, T):
        for n in range(N):
            seq_probs = V[:,t-1] * A[:,n] * B[n, obs_seq[t]]
            prev[t-1,n] = np.argmax(seq_probs)
            V[n,t] = np.max(seq_probs)

    return V, prev

def build_viterbi_path(prev, last_state):
    """Returns a state path ending in last_state in reverse order.
    最优路径回溯
    """
    T = len(prev)
    yield(last_state)
    for i in range(T-1, -1, -1):
        yield(prev[i, last_state])
        last_state = prev[i, last_state]
        
def observation_prob(obs_seq):
    """ P( entire observation sequence | A, B, pi ) """
    return np.sum(forward(obs_seq)[:,-1])

def state_path(obs_seq, A, B, pi):
    """
    Returns
    -------
    V[last_state, -1] : float
        Probability of the optimal state path
    path : list(int)
        Optimal state path for the observation sequence
    """
    V, prev = viterbi(obs_seq, A, B, pi)
    # Build state path with greatest probability
    last_state = np.argmax(V[:,-1])
    path = list(build_viterbi_path(prev, last_state))

    return V[last_state,-1], reversed(path)
```

继续1.1.3中的实例，根据事先学习得到的模型参数进行预测：

```python
states_out = state_path(observations_data, newA, newB, newpi)[1]
p = 0.0
for s in states_data:
    if next(states_out) == s: 
        p += 1

print(p / len(states_data))
```

求解得准确率为0.54

使用Viterbi算法计算病人的病情以及相应的概率：

```python
A = convert_map_to_matrix(transition_probability, states_label2id, states_label2id)
B = convert_map_to_matrix(emission_probability, states_label2id, observations_label2id)
observations_index = [observations_label2id[o] for o in observations]
pi = convert_map_to_vector(start_probability, states_label2id)
V, p = viterbi(observations_index, newA, newB, newpi)
print(" " * 7, " ".join(("%10s" % observations_id2label[i]) for i in observations_index))
for s in range(0, 2):
    print("%7s: " % states_id2label[s] + " ".join("%10s" % ("%f" % v) for v in V[s]))
print('\nThe most possible states and probability are:')
p, ss = state_path(observations_index, newA, newB, newpi)
for s in ss:
    print(states_id2label[s])
print(p)
```

求解结果为：

```
            normal       cold      dizzy
Healthy:   0.140000   0.022400   0.004480
  Fever:   0.140000   0.022400   0.004480

The most possible states and probability are:
Healthy
Healthy
Healthy
0.00448
```

## 2. 作业问题回答

### 2. 1 输入输出的格式是什么？

由1.1.3的实例可知。输入数据包括具体观测数据O（向量），具体状态数据I（向量）。输出数据显然是预测的状态数据（向量）。除此之外，输入时还有训练模型所必须的参数：对应状态集合Q（向量），对应观测集合V（向量），初始状态概率向量π（向量），状态转移矩阵A（矩阵），观测概率矩阵B（矩阵），

### 2. 2 需要对输入数据集做什么处理？

1. 对于输入的观测序列，通常需要进行预处理，如去除噪声、标准化等操作，以确保输入数据的可靠性和一致性。此外，还需要根据具体问题进行特征工程，提取有用的特征。
2. 需要将时间序列数据转换为适合HMM模型的格式，也就是将数据转换为观测值矩阵，其中每一行表示一个时间点，每一列表示一个可能的观测值。此外，如果观测值是连续数值，我们可能需要对其进行离散化处理；反之，如果观测值是离散数值，我们可能需要对其进行编码处理。
3. 需要将处理好的数据集划分为训练集和测试集，训练集用于训练HMM模型，测试集则用于评估模型的预测性能，确保的模型不仅在训练数据上表现良好，同时也能对未知的数据做出准确的预测。

### 2. 3 有哪些可用的库/包？｜有哪些对应的封装函数？

下面分别分python和matlab进行介绍：

#### Python

- `hmmlearn`：一个基于`Scikit-learn`的用于隐马尔科夫模型的库

  *常用函数：*

  - `GaussianHMM`：使用高斯分布建模的隐马尔科夫模型
  - `MultinomialHMM`：使用多项式分布建模的隐马尔科夫模型
  - `GMMHMM`：使用高斯混合模型建模的隐马尔科夫模型

- `pyhsmm`：一个用于隐马尔科夫模型和隐马尔科夫隐性层级模型的库

- `pomegranate`：一个用于构建概率图模型的库，包括隐马尔科夫模型

  - `HiddenMarkovModel`：通用的隐马尔科夫模型类
  - `HMM`：基于多元高斯分布的隐马尔科夫模型类


#### Matlab

- `Hidden Markov Models (HMM) Toolbox`：`Matlab`自带的用于隐马尔科夫模型的工具箱

  *常用函数：*

  - `hmmdecode`：用于计算观测序列的似然概率和状态序列的后验概率
  - `hmmestimate`：用于从已知观测序列和状态序列中估计模型参数

- `Pattern Recognition Toolbox`：`Matlab`中的模式识别工具箱，包含了对隐马尔科夫模型的支持

  *常用函数：*

  - `hmmtrain`：用于从已知观测序列和状态序列中训练模型参数
  - `hmmdecode`：用于计算观测序列的似然概率和状态序列的后验概率

  
### 2.4 预测效果如何？

在实例1.1.3中，即使在数据完全随机初始化的情况下，模型依然具有0.54的正确率，由此可见在实际操作中，HMM模型的预测准确率较高


### 2. 5 有哪些可能影响预测效果的因素？

预测效果受多个因素影响，以下是其中一些可能的因素：

- `数据质量`：输入数据的噪声水平和缺失程度都会对预测效果产生影响。
- `模型选择`：不同类型的隐马尔科夫模型假设了不同的数据分布，选择合适的模型对于预测效果至关重要。
- `特征工程`：提取有意义的特征可以帮助模型更好地捕捉数据的规律和模式。
- `数据量`：更多的数据可以帮助模型学习更准确的统计规律。
- `模型参数`：模型的初始化和调优对结果影响较大，需要合适的参数设置和良好的训练过程。